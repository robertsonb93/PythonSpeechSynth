import sys
import vtlPythonAPI as vtl
import tensorflow as tf
import TextScraper as sc
import TrainingDataGen as tdg
import matplotlib.pyplot as plt
import time

spkr ="test1.speaker"
maxLen = 0.005 #if they all come out at max length it will be 20cm
minLen = 0.0025 #if they all come out at min length it will be 10cm
paramset = tdg.generateValues(spkr,10,2,3,minLen,maxLen)

count = 0
t_start = time.process_time()
avg = 0
for p in paramset:
    count = count +1

    start = time.process_time()
    trainSet = tdg.generateAudio(p,spkr)
    t = time.process_time()-start
    avg = (t+ (count-1)*avg)/count
    print("It took ",t, "to finish one synthesis. Estimated ",avg*(len(paramset)-count), " seconds remaining.")

t_end = time.process_time()
print("Total Training generation time = ",t_end-t_start)



#lets see if we can get it to match a single sound.
#the outputs from the learner are:

#   Values for each vocalTractParam
#   Values for each starting state GlottisParam
#   Number of Frames for the sound
#   How fast the Frames play
#   Lengths for each tube
#   Distance from the glottis to the incisors (front teeth)
#   Then area in cm2 of the velum
#   The End state Glottis paramns

#The input to the learner would be the desired sound

#The feedback/error is what is generated by the VTL model from the networks given parameters.








        

