import sys
import vtlPythonAPI as vtl
import tensorflow as tf
import numpy as np
import TextScraper as sc
import TrainingDataGen as tdg
import matplotlib.pyplot as plt
import time
import csv



#********************************* DEFINITIONS************************************#
#*********************************************************************************#


def writeCSV(trainSet):
    with open('Train.csv', 'w', newline='') as csvfile:
        writer = csv.writer(csvfile, delimiter=',',
                            quotechar='|', quoting=csv.QUOTE_MINIMAL)
        for t in trainSet:
            gltSize = int(len(t.glottis)/t.numFrames)
            vSize = int(len(t.vtp)/t.numFrames)
            tSize = int(len(t.tubeLengths)/t.numFrames)
            #Incisor and velum are size numFrames
            for n in range(t.numFrames-1):
                glt = [str(g) for g in t.glottis[n*gltSize:(n+1)*gltSize]]
                gltEnd = [str(g) for g in t.glottis[(n+1)*gltSize:(n+2)*gltSize]]
                vtp = [str(v) for v in t.vtp[n*vSize:(n+1)*vSize]]
                vtpEnd = [str(v) for v in t.vtp[(n+1)*vSize:(n+2)*vSize]]
                tbel = [str(tl) for tl in t.tubeLengths[(n)*tSize:(n+1)*tSize]]
                tbelEnd= [str(tl) for tl in t.tubeLengths[(n+1)*tSize:(n+2)*tSize]]
                inc = [str(t.incisor[n])]
                incEnd = [str(t.incisor[n+1])]
                vel = [str(t.velum[n])]
                velEnd = [str(t.velum[n+1])]
                aud = [str(a) for a in t.audio[n*t.samplesPerFrame:(n+1)*t.samplesPerFrame]]
                
                row = glt + gltEnd + vtp + vtpEnd + tbel + tbelEnd + inc + incEnd + vel + velEnd + aud
                writer.writerow(row)
        print("Completed Writing Csv")


def newTrainingData(examples,frameRate,numFrames,spkr):
    maxLen = 0.005 #if they all come out at max length it will be 20cm
    minLen = 0.0025 #if they all come out at min length it will be 10cm
    minFrames = numFrames # min is 2 frames (start and end of an audio)
    maxFrames = numFrames

    #Produce the parameters we use for the generating speech.
    paramset = tdg.generateValues(spkr,examples,minFrames,maxFrames,frameRate,minLen,maxLen)

    trainSet = list()
    count = 0
    t_start = time.process_time()
    avg = 0
    for p in paramset:
        count = count +1
        start = time.process_time() 
        #Produce each parameter set into an audio file so we know the outcome of it.

        trainSet.append(tdg.generateAudio(p,spkr))

        t = time.process_time()-start
        avg = (t+ (count-1)*avg)/count
        print("It took ",t, "to finish one synthesis. Estimated ",avg*(len(paramset)-count), " seconds remaining.")

    t_end = time.process_time()
    print("Total Training generation time = ",t_end-t_start)
    writeCSV(trainSet)


def weight_variable(shape):
  initial = tf.random_normal(shape, stddev=0.1)
  return tf.Variable(initial)

#I hope this is correct, Review later if the phi is needed at eachs step or not.
def forwardprop(X, W):

    h    = tf.nn.sigmoid(tf.matmul(X, W[0]))  # The \sigma function
    for w in W[1:len(W)-1]:
        h = tf.nn.sigmoid(tf.matmul(h,w))

    yhat = tf.matmul(h, W[len(W)-1])  # The \varphi function
    return yhat



#******************************END DEFINITIONS************************************#
#*********************************************************************************#


#the outputs from the learner are:

#Number of frames for the sound
#Frames per Second
#The Glottis (list of  length (numGlottisParams * numFrames))
#The Vocal Tract Parameters we start from (list of length(numVocalTractParams * numFrames))
#The Length of tubes in the Synth model (list of length (tubeSections * num Frames))
#The distances from glottis to incisors (list of length(numFrames))
#The areas of the velum (list of of length(numFrames))

#The input to the learner would be the desired sound
#The feedback/error is what is generated by the VTL model from the networks given parameters.


spkr ="test1.speaker"
framerate = 4
numFrames = 3

newTrainingData(10000,framerate,numFrames,spkr)

#lets see if we can get it to match a single sound. with only a single frame transition of audio.
vtl.initSpeaker(spkr,False)
[srate,tubecount,vtcount,glotcount] = vtl.getSpeakerConstants()
inSize = int(srate/(framerate)) #I know this works with frameRate of 4
#inSize = trainSet[0].samplesPerFrame

#Glottis + vtp + tubeLengths + incisor + velum
outSize = (glotcount + vtcount + tubecount + 2) * 2#Times two for a start and end

#Here We give TF a list of the files it can read to get our values from
filename_queue = tf.train.string_input_producer(["Train.csv"])
reader = tf.TextLineReader()
key, value = reader.read(filename_queue)

record_defaults = [[1.0] for x in range(outSize + inSize )]
#Features are the last inSize columns(the audio)
row = tf.decode_csv(value, record_defaults=record_defaults)
audio = tf.stack(row[outSize:outSize+inSize])
params = tf.stack(row[0:outSize])

t_start = time.process_time()
entrpy = list()
W = list()
with tf.Session() as sess:
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)

    inputAudio = tf.placeholder(tf.float32,shape=[None,inSize])
    outActual = tf.placeholder(tf.float32,shape=[None,outSize])

    #create Weights (this is basically the network)
    h_size = 256
    W.append(weight_variable((inSize,h_size)))
    for s in range(2):
        W.append(weight_variable((h_size,h_size)))
    W.append(weight_variable((h_size,outSize)))

    #Forward propagation(How to calculate from input to output)
    outEstimate = forwardprop(inputAudio, W)

    #Back Progoation(How to correct the Error from the estimate)
    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=outActual, logits=outEstimate))
    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)


    init = tf.global_variables_initializer()
    sess.run(init)

    for i in range(10):  #It seems this will stop on the min(number of lines or range)    
        inp,outp = sess.run([audio,params])
        outp = np.reshape(outp,(1,outSize))
        inp = np.reshape(inp,(1,inSize))

        sess.run(train_step, feed_dict={inputAudio: inp, outActual: outp})
        entrpy.append(sess.run(cross_entropy, feed_dict={inputAudio: inp, outActual: outp}))
                     
      
t_end = time.process_time()
print("Total Training Network time = ",t_end-t_start)
plt.plot(entrpy)
plt.show(entrpy)








        

